{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc5fc1f-cffc-4a94-88ab-f2ff7794094f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks loaded  !!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, timedelta\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from conversight import Cluster\n",
    "from conversight import Dataset, Flow, Parameter, SmartAnalytics,task, ProactiveInsights, Dataset,TaskLibrary,FlowLibrary\n",
    "from csContext import CSContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsk = TaskLibrary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61feb4a-2236-4835-b237-7e35505ea6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def get_df(ctx: CSContext, query:str,ds_id:str)->pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform this function to get the data from db\n",
    "    Input : query -> sql query\n",
    "            ds_id -> cs dataset id\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from conversight import Dataset,Context\n",
    "        #ctx=Context()\n",
    "        \n",
    "        ctx.log.info(\"fetching data from db ->\")\n",
    "        query = str(query)\n",
    "        query = \" \".join(line.strip() for line in query.splitlines())\n",
    "        query = query.replace(\"(#)\", \"#\").replace(\"(\\n#\\n)\",\"#\").replace(\"(\\n#)\",\"#\").replace(\"(#\\n)\",\"#\").replace(\"(#\\n    )\",\"#\").replace(\"(\\n    #)\",\"#\").replace(\"( #)\",\"#\").replace(\"(# )\",\"#\").replace(\"(  #)\",\"#\").replace(\"(#  )\",\"#\") \n",
    "        ds = Dataset(ds_id)\n",
    "        df = ds.sqlDf(query)\n",
    "        ctx.log.info(\"Dataframe was fetched successfully and its shape was {}\".format(df.shape))\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        \n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9f037e-755d-43f7-80b4-8d6a5ab460b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31;1m[2024-03-22 07:52:09,130] [ERROR] This version of code is already available on version 0.1, use setVersion(0.1) from TaskLibrary to avail the code\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "get_df.register(libraryName = \"t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0154fa55-7462-4447-bf25-b5fcd48d3379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks loaded  !!\n"
     ]
    }
   ],
   "source": [
    "tsk.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5549cba-deb2-40f8-9503-a8e27fcdea42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31;1m[2024-03-22 07:52:12,354] [ERROR] Version 0.1 for the Task get_df is already in level O, these are the levels available for you to promote => [S, P, O, U]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tsk.t1.get_df.promote(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05408ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsk.t1.get_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c247ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def get_parameter(ctx: CSContext, df: pd.core.frame.DataFrame, key_columns_name: str, date_column_name: str, value_column_name: str, period: int, F_Mode:str,\n",
    "                 outlier_flag_by_product: bool, imputation_method: str, outlier_threshold: str, global_smoothing: bool)-> dict:\n",
    "    \"\"\"\n",
    "    Write the doc string here, Refer Forecasting Model task\n",
    "    pass outlier_threshold value as a str\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        import datetime as dt\n",
    "        from conversight import Dataset,Context\n",
    "        #ctx=Context()\n",
    "        ctx.log.info(\"get_parameter is started ->\")\n",
    "        \n",
    "        fieldMapping = {\"date_column\":date_column_name, \"value_column\":value_column_name}\n",
    "        threshold = [0,100]\n",
    "        forecast_type = \"M\"\n",
    "        forecast_period = period\n",
    "        outlier_threshold = float(outlier_threshold)\n",
    "\n",
    "        def impute_outliers_by_product(df, key_column, dat_column, value_column, imputation_value=\"median\", threshold=1.5):\n",
    "            ctx.log.info(f\"Outliers deduction by Product method {imputation_value} with threshold {threshold}\")\n",
    "            def impute_outliers_group(group):\n",
    "                group[value_column] = group[value_column].apply(lambda x: max(0, x))\n",
    "                Q1 = group[value_column].quantile(0.25)\n",
    "                Q3 = group[value_column].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - threshold * IQR\n",
    "                upper_bound = Q3 + threshold * IQR\n",
    "                outliers_mask = (group[value_column] < lower_bound) | (group[value_column] > upper_bound)\n",
    "                if imputation_value.lower() == \"mean\":\n",
    "                    imputed_value = group[value_column].mean()\n",
    "                elif imputation_value.lower() == \"median\":\n",
    "                    imputed_value = group[value_column].median()\n",
    "                elif imputation_value.lower() == \"mode\":\n",
    "                    imputed_value = group[value_column].mode().iloc[0]\n",
    "                elif \"qv\" in imputation_value.lower():\n",
    "                    imputation_value_qv = imputation_value.split(\"=\")[1]\n",
    "                    qv = float(imputation_value_qv)\n",
    "                    qv = group[value_column].quantile(qv)\n",
    "                    imputed_value = qv\n",
    "                else:\n",
    "                    iv = float(imputation_value)\n",
    "                    imputed_value = iv\n",
    "                group[value_column] = group[value_column].mask(outliers_mask, imputed_value)\n",
    "                return group\n",
    "            ctx.log.info(\"Identifying and imputing outliers..\")\n",
    "            df_sorted = df.sort_values(by=dat_column)\n",
    "            df_imputed = df_sorted.groupby(key_column, observed=True).apply(impute_outliers_group)\n",
    "            return df_imputed\n",
    "\n",
    "        def impute_outliers(df, column_name, imputation_value=\"median\", threshold=1.5):\n",
    "            ctx.log.info(f\"Outliers deduction method {imputation_value} with threshold {threshold}\")\n",
    "            df[column_name] = df[column_name].apply(lambda x: max(0, x))\n",
    "            Q1 = df[column_name].quantile(0.25)\n",
    "            Q3 = df[column_name].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]\n",
    "            if imputation_value.lower() == \"mean\":\n",
    "                imputed_value = df[column_name].mean()\n",
    "            elif imputation_value.lower() == \"median\":\n",
    "                imputed_value = df[column_name].median()\n",
    "            elif imputation_value.lower() == \"mode\":\n",
    "                imputed_value = df[column_name].mode().iloc[0]\n",
    "            elif \"qv\" in imputation_value.lower():\n",
    "                imputation_value_qv = imputation_value.split(\"=\")[1]\n",
    "                qv = float(imputation_value_qv)\n",
    "                qv = df[column_name].quantile(qv)\n",
    "                imputed_value = qv\n",
    "            else:\n",
    "                iv = float(imputation_value)\n",
    "                imputed_value = iv\n",
    "            ctx.log.info(\"Identifying and imputing outliers..\")\n",
    "            df[column_name] = df[column_name].mask((df[column_name] < lower_bound) | (df[column_name] > upper_bound), imputed_value)\n",
    "            return df\n",
    "        \n",
    "        columns = key_columns_name.split(\",\")\n",
    "        keys = \"key_column\"\n",
    "        df[keys] = df[columns].astype(str).agg(\"_\".join, axis=1)\n",
    "        columns.append(keys)\n",
    "        columns.extend([date_column_name, value_column_name])\n",
    "        df = df.dropna(subset=columns).reset_index(drop=True)\n",
    "    \n",
    "        df = df[[fieldMapping[\"date_column\"], fieldMapping[\"value_column\"] ,keys]]\n",
    "        df[keys] = df[keys].astype(str)\n",
    "        df[fieldMapping[\"date_column\"]] = pd.to_datetime(df[fieldMapping[\"date_column\"]]).dt.floor(\"D\")\n",
    "        df[fieldMapping[\"date_column\"]] =  df[fieldMapping[\"date_column\"]].astype(\"datetime64\")\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "        if F_Mode.lower() == \"month\":\n",
    "            df[fieldMapping[\"date_column\"]] = ((df[fieldMapping[\"date_column\"]] + pd.offsets.MonthEnd(0) - pd.offsets.MonthBegin(1)).dt.floor(\"D\"))\n",
    "            df = df.groupby([fieldMapping[\"date_column\"], keys], observed=True)[fieldMapping[\"value_column\"]].sum().reset_index()\n",
    "        elif F_Mode.lower() == \"week\":\n",
    "            df[fieldMapping[\"date_column\"]] = pd.to_datetime(df[fieldMapping[\"date_column\"]]).dt.floor(\"D\")\n",
    "            df[fieldMapping[\"date_column\"]] = df[fieldMapping[\"date_column\"]].dt.to_period(\"W\").apply(lambda r: r.start_time)\n",
    "            df = df.groupby([fieldMapping[\"date_column\"], keys], observed=True)[fieldMapping[\"value_column\"]].sum().reset_index()\n",
    "        df[fieldMapping[\"value_column\"]] = df[fieldMapping[\"value_column\"]].apply(lambda x: max(0, x))\n",
    "\n",
    "        if F_Mode.lower() == \"month\":\n",
    "          P = dt.datetime.today().replace(day=1).strftime(\"%Y-%m-%d\")\n",
    "          df = df[df[fieldMapping[\"date_column\"]] < P].reset_index(drop=True)\n",
    "        elif F_Mode.lower() == \"week\":\n",
    "          current_date = dt.datetime.today()\n",
    "          P = (current_date - dt.timedelta(days=current_date.weekday())).strftime(\"%Y-%m-%d\") #current week start monday\n",
    "          df = df[df[fieldMapping[\"date_column\"]] < P].reset_index(drop=True)\n",
    "\n",
    "        if outlier_flag_by_product and global_smoothing:\n",
    "            ctx.log.error(\"Both variables are True, set according...\")\n",
    "            return {\"status\": \"Failed\", \"message\": \"processing failed due to bad configuration\"}\n",
    "        else:\n",
    "            if outlier_flag_by_product:\n",
    "                ctx.log.info(\"outlier_flag_by_product is started ->\")\n",
    "                df_model = impute_outliers_by_product(df,keys,date_column_name,value_column_name,imputation_method,outlier_threshold)\n",
    "            if global_smoothing:\n",
    "                ctx.log.info(\"global_smoothing is started ->\")\n",
    "                df_model = impute_outliers(df,value_column_name,imputation_method,outlier_threshold)\n",
    "        if not outlier_flag_by_product and not global_smoothing:\n",
    "            df_model = df\n",
    "        \n",
    "        parameter ={\"MAPPING\":fieldMapping, \"ID\":keys, \"CAPPING\":threshold, \"F_TYPE\":forecast_type, \"F_PERIOD\":forecast_period, \"DATA\":df, \"DATA_MODEL\":df_model}\n",
    "        ctx.log.info(\"get_parameter is ended ->\")\n",
    "        return parameter\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"status\": \"Failed\", \"message\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be2ad026-cbe6-4b48-9137-74ef7102049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31;1m[2024-03-22 07:52:15,014] [ERROR] This version of code is already available on version 0.4, use setVersion(0.4) from TaskLibrary to avail the code\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "get_parameter.register(libraryName = \"t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0c69227-a1fe-4eec-9ff0-e74b40b96856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks loaded  !!\n"
     ]
    }
   ],
   "source": [
    "tsk.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12cd1fef-8cec-4fb0-9759-b78f230416af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31;1m[2024-03-22 07:52:18,814] [ERROR] Version 0.4 for the Task get_parameter is already in level O, these are the levels available for you to promote => [S, P, O, U]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tsk.t1.get_parameter.promote(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4382da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsk.t1.get_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3652c584-aa0b-4967-942d-d5c73804fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def Forecasting_day(ctx: CSContext, p : dict, allow_Forecast: bool, dataset_id: str, data_level : str, table_name : str, AutoArimaTable : str, flag : str, select_model : str, F_Mode:str,\n",
    "                    reRun:bool, reRunModel:str):\n",
    "    \"\"\"Forecasting day/week/month level\"\"\"\n",
    "    try:\n",
    "        import os\n",
    "        import sys\n",
    "        import re\n",
    "        import traceback\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        # import ray \n",
    "        # ray.init(ignore_reinit_error=True)\n",
    "        from conversight import TaskPool\n",
    "        tp = TaskPool(pool_size=480)\n",
    "        import datetime as dt\n",
    "        from conversight import Dataset,Context\n",
    "        #ctx = Context()\n",
    "\n",
    "\n",
    "        class suppress_logs(object):\n",
    "            \"\"\"\n",
    "            A context manager for doing a \"deep suppression\" of stdout and stderr in\n",
    "            Python, i.e. will suppress all print, even if the print originates in a\n",
    "            compiled C/Fortran sub-function.\n",
    "            This will not suppress raised exceptions, since exceptions are printed\n",
    "            to stderr just before a script exits, and after the context manager has\n",
    "            exited (at least, I think that is why it lets exceptions through).\n",
    "            \"\"\"\n",
    "            def __init__(self):\n",
    "                # Open a pair of null files\n",
    "                self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n",
    "                # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "                self.save_fds = [os.dup(1), os.dup(2)]\n",
    "            def __enter__(self):\n",
    "                # Assign the null pointers to stdout and stderr.\n",
    "                os.dup2(self.null_fds[0], 1)\n",
    "                os.dup2(self.null_fds[1], 2)\n",
    "            def __exit__(self, *_):\n",
    "                # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "                os.dup2(self.save_fds[0], 1)\n",
    "                os.dup2(self.save_fds[1], 2)\n",
    "                # Close the null files\n",
    "                for fd in self.null_fds + self.save_fds:\n",
    "                    os.close(fd)\n",
    "\n",
    "\n",
    "        fieldMapping = p[\"MAPPING\"]\n",
    "        keys = p[\"ID\"]\n",
    "        threshold = p[\"CAPPING\"]\n",
    "        forecast_type = p[\"F_TYPE\"]\n",
    "        forecast_period = p[\"F_PERIOD\"]\n",
    "        df_act = p[\"DATA\"]\n",
    "        df_model = p[\"DATA_MODEL\"]\n",
    "        select_model = select_model.split(\",\")\n",
    "        ctx.log.info(f\"Parameters collected successfully and selected model : {select_model} and forecast mode : {F_Mode.lower()}\")\n",
    "        \n",
    "        if F_Mode.lower() == \"day\":\n",
    "            df_model[fieldMapping[\"date_column\"]] = pd.to_datetime(df_model[fieldMapping[\"date_column\"]]).dt.floor(\"D\")\n",
    "            ctx.log.info(\"Datetime convertion successfull\")\n",
    "            max_date = str(df_model[fieldMapping[\"date_column\"]].max())\n",
    "            dateprocess_max = dt.datetime.strptime(max_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "            min_date = str(df_model[fieldMapping[\"date_column\"]].min())\n",
    "            dateprocess_min = dt.datetime.strptime(min_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "            max_date_limit = (dateprocess_max - pd.DateOffset(days=1)).strftime(\"%Y-%m-%d\")\n",
    "            min_date_limit = str(dateprocess_min.year) +\"-\"+ str(dateprocess_min.month) +\"-\"+ str(dateprocess_min.day)\n",
    "            \n",
    "        elif F_Mode.lower() == \"week\":\n",
    "            df_model[fieldMapping[\"date_column\"]] = pd.to_datetime(df_model[fieldMapping[\"date_column\"]]).dt.floor(\"D\")\n",
    "            #df_model[fieldMapping[\"date_column\"]] = df_model[fieldMapping[\"date_column\"]].dt.to_period(\"W\").apply(lambda r: r.start_time)\n",
    "            ctx.log.info(\"Datetime convertion successfull\")\n",
    "            max_date = str(df_model[fieldMapping[\"date_column\"]].max())\n",
    "            dateprocess_max = dt.datetime.strptime(max_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "            min_date = str(df_model[fieldMapping[\"date_column\"]].min())\n",
    "            dateprocess_min = dt.datetime.strptime(min_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "            # max_date_limit = (dateprocess_max - pd.DateOffset(weeks=1)).strftime(\"%Y-%m-%d\")\n",
    "            max_date_limit = str(dateprocess_max.year) +\"-\"+ str(dateprocess_max.month) +\"-\"+ str(dateprocess_max.day)\n",
    "            min_date_limit = str(dateprocess_min.year) +\"-\"+ str(dateprocess_min.month) +\"-\"+ str(dateprocess_min.day)\n",
    "                \n",
    "        elif F_Mode.lower() == \"month\":\n",
    "            df_model[fieldMapping[\"date_column\"]] = pd.to_datetime(df_model[fieldMapping[\"date_column\"]]).dt.floor(\"D\")\n",
    "            ctx.log.info(\"Datetime convertion successfull\")\n",
    "            max_date = str(df_model[fieldMapping[\"date_column\"]].max())\n",
    "            dateprocess_max = dt.datetime.strptime(max_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "            min_date = str(df_model[fieldMapping[\"date_column\"]].min())\n",
    "            dateprocess_min = dt.datetime.strptime(min_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "            # max_date_limit = (dateprocess_max - pd.DateOffset(months=1)).strftime(\"%Y-%m-01\")\n",
    "            max_date_limit = (dateprocess_max).strftime(\"%Y-%m-01\")\n",
    "            min_date_limit = (dateprocess_min).strftime(\"%Y-%m-01\")\n",
    "        \n",
    "        ctx.log.info(\"Min_date_limit and Max_date_limit calculated successfully\")\n",
    "        ctx.log.info(f\"Min - Data --> {min_date_limit} and Max - Data --> {max_date_limit}\")\n",
    "        \n",
    "        def Heartbeat( df):\n",
    "            n = pd.DataFrame(df.fcst)\n",
    "            n[\"shift\"] = n.shift(1)\n",
    "            n = n.fillna(0)\n",
    "            n.columns = [\"ori\",\"shift\"]\n",
    "            n[\"trend\"] = n[\"shift\"].astype(float) - n[\"ori\"].astype(float)\n",
    "            n[\"t\"] = n[\"trend\"]*(-1)\n",
    "            n.t = np.where(n.t == -0.0, 0.0,n.t)\n",
    "            return list(n[\"t\"])\n",
    "\n",
    "        def Percentage_change(df):\n",
    "            n = pd.DataFrame(df.fcst)    \n",
    "            n[\"s\"] = n.fcst.shift(1)\n",
    "            n = n.fillna(0.0)\n",
    "            n[\"perc\"] = ((n[\"fcst\"] - n[\"s\"])/n[\"s\"])*100\n",
    "            n = n.fillna(0.0)\n",
    "            #n[\"perc\"][0] = \"initial\"\n",
    "            return n[\"perc\"]\n",
    "        \n",
    "        def TrendCal( df):\n",
    "            def linreg(X, Y):\n",
    "                N = len(X)\n",
    "                Sx = Sy = Sxx = Syy = Sxy = 0.0\n",
    "                for x, y in zip(X, Y):\n",
    "                    Sx = Sx + x\n",
    "                    Sy = Sy + y\n",
    "                    Sxx = Sxx + x*x\n",
    "                    Syy = Syy + y*y\n",
    "                    Sxy = Sxy + x*y\n",
    "                det = Sxx * N - Sx * Sx\n",
    "                return (Sxy * N - Sy * Sx)/det, (Sxx * Sy - Sx * Sxy)/det\n",
    "\n",
    "            a,b = linreg(range(len(df.fcst)),df.fcst)\n",
    "            x = np.arange(len(df.fcst))\n",
    "            y = a*x+b\n",
    "            return y\n",
    "        \n",
    "        def missed_data_optimized(df, keys, fieldMapping, min_date, max_date, F_Mode):\n",
    "            df[fieldMapping[\"date_column\"]] = pd.to_datetime(df[fieldMapping[\"date_column\"]]).dt.floor(\"D\")\n",
    "            if F_Mode.lower() == \"month\":\n",
    "                dates_required = pd.date_range(min_date, max_date, freq=\"MS\").strftime(\"%Y-%m-01\").tolist()\n",
    "            elif F_Mode.lower() == \"week\":\n",
    "                dates_required = pd.date_range(min_date, max_date, freq=\"W-MON\").strftime(\"%Y-%m-%d\").tolist()\n",
    "            dates_required = pd.DataFrame(dates_required, columns=[fieldMapping[\"date_column\"]])\n",
    "            dates_required[fieldMapping[\"date_column\"]] = pd.to_datetime(dates_required[fieldMapping[\"date_column\"]]).dt.floor(\"D\")\n",
    "            unique_keys = df[keys].unique()\n",
    "            all_combinations = pd.MultiIndex.from_product([unique_keys, dates_required[fieldMapping[\"date_column\"]]], names=[keys, fieldMapping[\"date_column\"]]).to_frame(index=False)\n",
    "            grouped = df.groupby([keys, fieldMapping[\"date_column\"]], observed=True)[fieldMapping[\"value_column\"]].sum().reset_index()\n",
    "            processed_data = all_combinations.merge(grouped, on=[keys, fieldMapping[\"date_column\"]], how=\"left\")\n",
    "            processed_data[fieldMapping[\"value_column\"]] = processed_data[fieldMapping[\"value_column\"]].fillna(0)\n",
    "            return processed_data\n",
    "        \n",
    "        def preprocessing(data,keys, fieldMapping,allow_forecast,min_date, max_date,F_Mode):\n",
    "            limiter = 0\n",
    "            if F_Mode.lower() == \"day\":\n",
    "                if allow_forecast == True:\n",
    "                    limiter = 30\n",
    "                else: limiter = 180\n",
    "            elif F_Mode.lower() == \"week\":\n",
    "                if allow_forecast == True:\n",
    "                    limiter = 4\n",
    "                else: limiter = 24\n",
    "            elif F_Mode.lower() == \"month\":\n",
    "                if allow_forecast == True:\n",
    "                    limiter = 1\n",
    "                else: limiter = 6\n",
    "\n",
    "            counts = data.groupby(keys, observed=True).size()\n",
    "            valid_items = counts[counts >= limiter].index\n",
    "            data = data[data[keys].isin(valid_items)]\n",
    "\n",
    "            processed_data = missed_data_optimized(data, keys, fieldMapping, min_date, max_date, F_Mode)\n",
    "            processed_data = processed_data.rename(columns={fieldMapping[\"date_column\"]: \"time\"})\n",
    "            return processed_data\n",
    "            \n",
    "\n",
    "        #@ray.remote\n",
    "        def forecastmodel1(df,fieldMapping,prod_id,keys,forecast_period,F_Mode,df_act,parameter_df):\n",
    "            warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "            df = df[df[keys] == prod_id].reset_index(drop=True)\n",
    "            df.drop([keys],axis = 1, inplace = True)\n",
    "\n",
    "            df_act = df_act[df_act[keys] == prod_id].reset_index(drop=True)\n",
    "            df_act.drop([keys],axis = 1, inplace = True)\n",
    "\n",
    "            try:\n",
    "                params = parameter_df[parameter_df[keys] == prod_id]\n",
    "                params = params.reset_index(drop=True)\n",
    "                seas_mode = params[\"seasonalMode\"][0]\n",
    "            except:\n",
    "                seas_mode = \"additive\"#\"multiplicative\"\n",
    "\n",
    "            try:\n",
    "                from kats.consts import TimeSeriesData as tsd\n",
    "                from kats.models.prophet import ProphetModel\n",
    "                from kats.models.prophet import ProphetParams\n",
    "                data = tsd(df)\n",
    "                parms = ProphetParams(seasonality_mode=seas_mode)\n",
    "                model = ProphetModel(data,parms)\n",
    "                with suppress_logs():\n",
    "                    model.fit()\n",
    "                if F_Mode.lower() == \"day\":\n",
    "                    forcast = model.predict(steps = forecast_period, freq=\"D\")\n",
    "                elif F_Mode.lower() == \"week\":\n",
    "                    forcast = model.predict(steps = forecast_period,freq=\"W-MON\")\n",
    "                elif F_Mode.lower() == \"month\":\n",
    "                    forcast = model.predict(steps = forecast_period,freq=\"MS\")\n",
    "            except:\n",
    "                from prophet import Prophet\n",
    "                df.columns = [\"ds\", \"y\"]\n",
    "                m = Prophet(seasonality_mode=seas_mode) #seasonality_mode=\"multiplicative\", scaling=\"minmax\", seasonality_prior_scale=12\n",
    "                with suppress_logs():\n",
    "                    m.fit(df)\n",
    "                if F_Mode.lower() == \"day\":\n",
    "                    future = m.make_future_dataframe(periods= forecast_period,freq=\"D\", include_history=False)\n",
    "                elif F_Mode.lower() == \"week\":\n",
    "                    future = m.make_future_dataframe(periods= forecast_period,freq=\"W-MON\", include_history=False)\n",
    "                elif F_Mode.lower() == \"month\":\n",
    "                    future = m.make_future_dataframe(periods= forecast_period,freq=\"MS\", include_history=False)\n",
    "                forcast = m.predict(future)\n",
    "                forcast = forcast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "                forcast.columns = [\"time\",\"fcst\",\"fcst_lower\",\"fcst_upper\"]\n",
    "            forcast[keys] = prod_id\n",
    "            forcast[\"actual_value\"] = 0.0\n",
    "            df_act.columns = [\"time\",\"actual_value\"]\n",
    "            df_act[\"fcst\"] = df_act[\"actual_value\"]\n",
    "            df_act[\"fcst_lower\"] = 0.0\n",
    "            df_act[\"fcst_upper\"] = 0.0\n",
    "            df_act[keys] = prod_id  \n",
    "            df_act = df_act[forcast.columns]  \n",
    "            df_act = pd.concat([df_act,forcast])\n",
    "            df_act[\"fcst\"] = df_act[\"fcst\"].round()\n",
    "            df_act[\"fcst\"] = np.where(df_act[\"fcst\"]<0,0.0,df_act[\"fcst\"]) \n",
    "            df_act[\"fcst\"] = np.where(df_act[\"fcst\"] == -0.0,0.0,df_act[\"fcst\"]) \n",
    "            t = TrendCal(df_act)\n",
    "            h = Heartbeat(df_act)\n",
    "            cp = Percentage_change(df_act)\n",
    "            df_act[\"trend\"] = t\n",
    "            df_act[\"heartbeat\"] = h  \n",
    "            df_act[\"change percentage\"] =  cp\n",
    "            return df_act\n",
    "        \n",
    "        \n",
    "        #@ray.remote\n",
    "        def forecastmodel2(df, fieldMapping, prod_id, keys, forecast_period,F_Mode,df_act,parameter_df):\n",
    "            from pandas.tseries.offsets import DateOffset\n",
    "            from statsmodels.tsa.stattools import adfuller\n",
    "            import statsmodels.api as sm\n",
    "            from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "            warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "\n",
    "            df = df[df[keys] == prod_id].reset_index(drop=True)\n",
    "            df.drop([keys],axis = 1, inplace = True)\n",
    "\n",
    "            df_act = df_act[df_act[keys] == prod_id].reset_index(drop=True)\n",
    "            df_act.drop([keys],axis = 1, inplace = True)\n",
    "            df_act.columns = [\"time\",fieldMapping[\"value_column\"]]\n",
    "\n",
    "            def adfuller_test(sales):\n",
    "                result=adfuller(sales)\n",
    "                if result[1] <= 0.05:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1\n",
    "            try:\n",
    "                Seasonality_flag = adfuller_test(df[fieldMapping[\"value_column\"]])\n",
    "            except:\n",
    "                Seasonality_flag = 0\n",
    "\n",
    "            try:\n",
    "                params = parameter_df[parameter_df[keys] == prod_id]\n",
    "                params = params.reset_index(drop=True)\n",
    "                p, q, P, Q, m, d, D = params[\"p\"][0],params[\"q\"][0],params[\"sP\"][0],params[\"sQ\"][0],params[\"m\"][0],params[\"d\"][0],params[\"sD\"][0]\n",
    "            except:\n",
    "                def seas_len(F_Mode):\n",
    "                    if F_Mode.lower().lower() == \"day\":\n",
    "                        mm = 365\n",
    "                    elif F_Mode.lower().lower() == \"week\":\n",
    "                        mm = 52\n",
    "                    elif F_Mode.lower().lower() == \"month\":\n",
    "                        mm = 12\n",
    "                    return mm\n",
    "                mm = seas_len(F_Mode)\n",
    "                if Seasonality_flag:\n",
    "                    p, q, P, Q, m, d, D = 1, 1, 1, 1, mm, 1, 1\n",
    "                else:\n",
    "                    p, q, P, Q, m, d, D = 1, 1, 1, 1, mm, 0, 0\n",
    "\n",
    "            #future_dates = [df.time[-1:] + DateOffset(months=x) for x in range(0, forecast_period + 1)]\n",
    "            if F_Mode.lower().lower() == \"day\":\n",
    "                future_dates = [df.time[-1:] + DateOffset(days=x) for x in range(0, forecast_period + 1)]\n",
    "            elif F_Mode.lower().lower() == \"week\":\n",
    "                future_dates = [df.time[-1:] + DateOffset(weeks=x) for x in range(0, forecast_period + 1)]\n",
    "            elif F_Mode.lower().lower() == \"month\":\n",
    "                future_dates = [df.time[-1:] + DateOffset(months=x) for x in range(0, forecast_period + 1)]\n",
    "            future_dates = pd.DataFrame(future_dates)\n",
    "            future_dates.columns = [\"time\"]\n",
    "            future_dates = future_dates.reset_index()\n",
    "            future_dates.drop([\"index\"], axis=1 , inplace=True)   \n",
    "            future_dates.drop(future_dates.index[0:1], axis=0, inplace=True)  \n",
    "\n",
    "            try:\n",
    "                model = sm.tsa.statespace.SARIMAX(df[fieldMapping[\"value_column\"]], order=(p, d, q),seasonal_order=(P, D, Q, m), verbose=False)\n",
    "                with suppress_logs():\n",
    "                    results = model.fit()\n",
    "            except:\n",
    "                model = sm.tsa.statespace.SARIMAX(df[fieldMapping[\"value_column\"]], order=(p, d, q),seasonal_order=(P, D, Q, m), initialization=\"approximate_diffuse\", verbose=False)\n",
    "                with suppress_logs():\n",
    "                    results = model.fit()\n",
    "\n",
    "            future_df = pd.concat([df_act,future_dates])\n",
    "            future_df = future_df.reset_index()\n",
    "\n",
    "            future_df[\"forecast\"] = results.predict(start = len(df), end = len(df) + forecast_period)  \n",
    "            future_df[[\"time\", fieldMapping[\"value_column\"], \"forecast\"]]\n",
    "            future_df[keys]  = prod_id\n",
    "\n",
    "            future_df.fillna(0,inplace = True)\n",
    "            future_df[\"fcst\"] =   future_df[\"forecast\"].astype(float) + future_df[fieldMapping[\"value_column\"]].astype(float)\n",
    "            future_df.drop([\"forecast\"],axis = 1, inplace = True) \n",
    "\n",
    "            future_df[\"fcst\"] = future_df[\"fcst\"].round()\n",
    "            future_df[\"fcst\"] = np.where(future_df[\"fcst\"] < 0, 0.0,future_df[\"fcst\"]) \n",
    "            future_df[\"fcst\"] = np.where(future_df[\"fcst\"] == -0.0, 0.0,future_df[\"fcst\"])\n",
    "\n",
    "            future_df[\"fcst_lower\"] = 0.0\n",
    "            future_df[\"fcst_upper\"] = 0.0\n",
    "\n",
    "            t = TrendCal(future_df)\n",
    "            h = Heartbeat(future_df)\n",
    "            cp = Percentage_change(future_df)\n",
    "            future_df[\"trend\"] = t\n",
    "            future_df[\"heartbeat\"] = h  \n",
    "            future_df[\"change percentage\"] =  cp  \n",
    "            future_df[\"change percentage\"] = future_df[\"change percentage\"].astype(float)\n",
    "            future_df = future_df.rename(columns={fieldMapping[\"value_column\"]: \"actual_value\"})\n",
    "            return future_df\n",
    "\n",
    "\n",
    "        ##### Main forecast execution start #######################################################\n",
    "        ctx.log.info(\"Preprocessing the Data\")   \n",
    "        prod_ids = df_model[keys].unique()\n",
    "        ctx.log.info(\"Getting All unique Materials to do Forecast\")\n",
    "        ctx.log.info(\"Total unique number {}\".format(len(prod_ids)))\n",
    "        #df_id = ray.put(df_model)\n",
    "\n",
    "        # frame, result = [], []\n",
    "        # cnt = 1\n",
    "        # for i in prod_ids:\n",
    "        #     tp.execute([df_model,keys,fieldMapping, i,allow_Forecast,min_date_limit,max_date_limit,F_Mode], preprocessing)\n",
    "        #     #frame.append(preprocessing.remote(df_id,keys,fieldMapping, i,allow_Forecast,min_date_limit,max_date_limit,F_Mode))\n",
    "        #     if cnt % 100 == 0:\n",
    "        #         result.extend(tp.collect_all()[1])\n",
    "        #     if cnt%1000 == 0:\n",
    "        #         ctx.log.info(\"Completed {} Items\".format(cnt))\n",
    "        #     cnt+=1\n",
    "        # #result = ray.get(frame)\n",
    "        # # Collect all results after the loop has finished\n",
    "        # remaining_results = tp.collect_all()[1]\n",
    "        # result.extend(remaining_results)\n",
    "\n",
    "        # result = pd.concat(result)\n",
    "        result = preprocessing(df_model,keys,fieldMapping,allow_Forecast,min_date_limit,max_date_limit,F_Mode)\n",
    "        ctx.log.info(\"Forecast Preprocessing Done\")\n",
    "        \n",
    "        prod_ids = result[keys].unique()\n",
    "        ctx.log.info(\"Total unique number for modeling {}\".format(len(prod_ids)))\n",
    "        #result_id = ray.put(result)\n",
    "        #df_act_id = ray.put(df_act)\n",
    "\n",
    "        #adding missing month with actuals\n",
    "        frame = result[[\"time\", keys]]\n",
    "        frame.columns = [fieldMapping[\"date_column\"], keys]\n",
    "        df_act = frame.merge(df_act, how=\"left\", on=[fieldMapping[\"date_column\"], keys])\n",
    "        df_act = df_act.fillna(0)\n",
    "\n",
    "        try:\n",
    "            ctx.log.info(\"AutoArima Table name: {}\".format(AutoArimaTable))\n",
    "            ds = Dataset(dataset_id)\n",
    "            parameter_df = ds.sqlDf(f\"\"\"select * from #{AutoArimaTable}\"\"\")\n",
    "            pattern = r\"^m_(.*)\"  # Match \"m_\" at the beginning, followed by any character sequence\n",
    "            parameter_df.columns = [re.sub(pattern, r\"\\1\", col) for col in parameter_df.columns]\n",
    "            ctx.log.info(\"Successfully collected AutoArima Parameter\")\n",
    "        except:\n",
    "            ctx.log.info(\"No such {} Table in db --> Taking default parameter\".format(AutoArimaTable))\n",
    "            parameter_df = pd.DataFrame()\n",
    "\n",
    "        ##Rerun Check\n",
    "        if reRunModel.lower() == \"model_1\":\n",
    "            m1reRun, m2reRun = reRun, False\n",
    "            select_model = [\"model_1\",\"model_2\",\"model_3\"]\n",
    "        elif reRunModel.lower() == \"model_2\":\n",
    "            m2reRun = reRun\n",
    "            select_model = [\"model_2\",\"model_3\"]\n",
    "        else:\n",
    "            m1reRun, m2reRun = False, False\n",
    "\n",
    "        if \"Model_1\" in select_model or \"model_1\" in select_model:\n",
    "            m1prod_ids = prod_ids\n",
    "            ctx.log.info(\"Model 1 Started ...\")\n",
    "            if m1reRun:\n",
    "                try:\n",
    "                    ctx.log.info(\"ReRun Model 1 Started ...\")\n",
    "                    reRunDf = ds.sqlDf(\"\"\"select * From #reRunForecastDummy\"\"\")\n",
    "                    reRunDf.columns = [re.sub(pattern, r\"\\1\", col) for col in reRunDf.columns]\n",
    "                    exclude_prod_ids = reRunDf[keys].unique()\n",
    "                    m1prod_ids = list(set(m1prod_ids)-set(exclude_prod_ids))\n",
    "                    ctx.log.info(f\"excluded item length: {len(exclude_prod_ids)} and rerun item length: {len(m1prod_ids)}\")\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "            frame1, cnt = [], 1\n",
    "            for i in m1prod_ids:\n",
    "                #frame1.append(forecastmodel1.remote(result_id,fieldMapping,i,keys,forecast_period,F_Mode,df_act,parameter_df))\n",
    "                tp.execute([result,fieldMapping,i,keys,forecast_period,F_Mode,df_act,parameter_df], forecastmodel1)\n",
    "                if cnt % 100 == 0:\n",
    "                    frame1.extend(tp.collect_all()[1])\n",
    "                if cnt%1000 == 0:\n",
    "                    ctx.log.info(\"Completed {} Items\".format(cnt))\n",
    "                if cnt%2000 == 0:\n",
    "                    result11 = pd.concat(frame1)\n",
    "                    ds.smartAnalytics.create(\"reRunForecastDummy\",result11,False,True,True)\n",
    "                cnt+=1\n",
    "\n",
    "            # Collect all results after the loop has finished\n",
    "            remaining_results = tp.collect_all()[1]\n",
    "            frame1.extend(remaining_results)\n",
    "            \n",
    "            #result1 = ray.get(frame1)\n",
    "            result1 = pd.concat(frame1)\n",
    "\n",
    "            if m1reRun:\n",
    "                result1 = pd.concat([result1, reRunDf], ignore_index=True)\n",
    "\n",
    "            # result1.columns = [\"Forecast_Date\",\"Forecasted_Value\",\"Forecast_Lower\",\"Forecast_Upper\",\n",
    "            #                    \"Forecast_Data_Level_Key\",\"Actual_Values\",\"Forecast_Trend\",\"Forecast_heartbeat\",\"Forecast_Change\"]\n",
    "            result1 = result1.rename(columns={\"time\": \"Forecast_Date\", \"fcst\": \"Forecasted_Value\", \"fcst_lower\": \"Forecast_Lower\",\n",
    "                                              \"fcst_upper\": \"Forecast_Upper\", keys: \"Forecast_Data_Level_Key\", \"actual_value\": \"Actual_Values\",\n",
    "                                              \"trend\": \"Forecast_Trend\", \"heartbeat\": \"Forecast_heartbeat\", \"change_percentage\": \"Forecast_Change\"})\n",
    "\n",
    "            result1[\"Forecast_Data_Level\"]= data_level\n",
    "            result1[\"Forecast_Data_Level_Key\"] = result1[\"Forecast_Data_Level_Key\"].str.strip(\"_\")\n",
    "            result1[\"Forecast_Method\"] = \"Model 1\"\n",
    "            result1[\"Forecast_Method_Version\"] = \"V1\"\n",
    "            result1[\"Forecasted_Flag\"] = \"Y\"\n",
    "            result1[\"Forcast_Explain\"] = \"\"\n",
    "              \n",
    "            result1[\"Forecast_Change\"] = 0.0\n",
    "            # result1[\"Forecast_Change\"].fillna(value=0, inplace=True)\n",
    "            # result1[\"Forecast_Change\"] = pd.to_numeric(result1[\"Forecast_Change\"], errors=\"coerce\")\n",
    "            # result1[\"Forecast_Change\"] = np.where(result1[\"Forecast_Change\"] == float(\"-inf\"),-1000.00,result1[\"Forecast_Change\"])\n",
    "            # result1[\"Forecast_Change\"] = np.where(result1[\"Forecast_Change\"] == float(\"inf\"),1000.00,result1[\"Forecast_Change\"])\n",
    "            # result1[\"Forecast_Change\"] = np.where(result1[\"Forecast_Change\"] > 1000,1000.00,result1[\"Forecast_Change\"])  \n",
    "            # result1[\"Forecast_Change\"] = np.where(result1[\"Forecast_Change\"]< -1000,-1000.00,result1[\"Forecast_Change\"])\n",
    "            # #result1[\"Forecast_Change\"] = np.clip(result1[\"Forecast_Change\"], -1000.00, 1000.00)\n",
    "\n",
    "\n",
    "            result1[\"Forecasted_Value\"] = np.where(result1[\"Forecasted_Value\"]<0,0,result1[\"Forecasted_Value\"])      \n",
    "\n",
    "            result1 = result1[[\"Forecast_Data_Level\",\"Forecast_Data_Level_Key\",\"Forecast_Method\",\"Forecast_Method_Version\",\"Forecasted_Flag\",\n",
    "                           \"Forecast_Date\",\"Actual_Values\", \"Forecasted_Value\",\"Forecast_Lower\", \"Forecast_Upper\",\"Forecast_Trend\",\n",
    "                         \"Forecast_heartbeat\",\"Forecast_Change\", \"Forcast_Explain\"]]\n",
    "            ctx.log.info(\"Model 1 Forecasting Done\")\n",
    "            ds = Dataset(dataset_id)  \n",
    "            ctx.log.info(\"Dataset Loaded\")\n",
    "            response = ds.smartAnalytics.create(table_name,result1,False,flag,True)\n",
    "            if \"status\" in response and response[\"status\"] == \"success\":\n",
    "                ctx.log.info(\"smart analytics creation is done -> {}\".format(dt.datetime.utcnow()))\n",
    "                ctx.log.info(\"Creation of table {} was done properly and smart analytics is created successfully\".format(table_name))\n",
    "                try:\n",
    "                    ds.smartAnalytics.delete(\"reRunForecastDummy\")\n",
    "                    ctx.log.info(\"Successfully deleted reRunForecastDummy Table\")\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                ctx.log.error(\"smart analytics creation is failed -> {}\".format(dt.datetime.utcnow()))\n",
    "                return {\"status\": \"failed\", \"message\": \"Organization processing failed due to an error {}\".format(response[\"message\"] if \"message\" in response else \"internalServerError\")}\n",
    "\n",
    "\n",
    "        if \"Model_2\" in select_model or \"model_2\" in select_model:\n",
    "            m2prod_ids = prod_ids\n",
    "            ctx.log.info(\"Model 2 Started ...\")\n",
    "            if m2reRun:\n",
    "                try:\n",
    "                    ctx.log.info(\"ReRun Model 2 Started ...\")\n",
    "                    reRunDf = ds.sqlDf(\"\"\"select * From #reRunForecastDummy\"\"\")\n",
    "                    reRunDf.columns = [re.sub(pattern, r\"\\1\", col) for col in reRunDf.columns]\n",
    "                    exclude_prod_ids = reRunDf[keys].unique()\n",
    "                    m2prod_ids = list(set(m2prod_ids)-set(exclude_prod_ids))\n",
    "                    ctx.log.info(f\"excluded item length: {len(exclude_prod_ids)} and rerun item length: {len(m2prod_ids)}\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            frame2, cnt = [], 1\n",
    "            for i in m2prod_ids:\n",
    "                #frame2.append(forecastmodel1.remote(result_id,fieldMapping,i,keys,forecast_period,F_Mode,df_act,parameter_df))\n",
    "                tp.execute([result,fieldMapping,i,keys,forecast_period,F_Mode,df_act,parameter_df], forecastmodel2)\n",
    "                if cnt % 100 == 0:\n",
    "                    frame2.extend(tp.collect_all()[1])\n",
    "                if cnt%1000 == 0:\n",
    "                    ctx.log.info(\"Completed {} Items\".format(cnt))\n",
    "                if cnt%2000 == 0:\n",
    "                    result22 = pd.concat(frame2)\n",
    "                    result22.drop([\"index\"],axis = 1, inplace = True)\n",
    "                    ds.smartAnalytics.create(\"reRunForecastDummy\",result22,False,True,True)\n",
    "                cnt+=1\n",
    "\n",
    "            # Collect all results after the loop has finished\n",
    "            remaining_results = tp.collect_all()[1]\n",
    "            frame2.extend(remaining_results)\n",
    "            \n",
    "            #result2 = ray.get(frame2)\n",
    "            result2 = pd.concat(frame2)\n",
    "            result2.drop([\"index\"],axis = 1, inplace = True)\n",
    "\n",
    "            if m2reRun:\n",
    "                result2 = pd.concat([result2, reRunDf], ignore_index=True)\n",
    "\n",
    "            # result2.columns = [\"Forecast_Date\", \"Actual_Values\", \"Forecast_Data_Level_Key\", \"Forecasted_Value\",\"Forecast_Lower\",\"Forecast_Upper\",\n",
    "            #                   \"Forecast_Trend\",\"Forecast_heartbeat\",\"Forecast_Change\"]\n",
    "            result2 = result2.rename(columns={\"time\": \"Forecast_Date\", \"fcst\": \"Forecasted_Value\", \"fcst_lower\": \"Forecast_Lower\",\n",
    "                                              \"fcst_upper\": \"Forecast_Upper\", keys: \"Forecast_Data_Level_Key\", \"actual_value\": \"Actual_Values\",\n",
    "                                              \"trend\": \"Forecast_Trend\", \"heartbeat\": \"Forecast_heartbeat\", \"change_percentage\": \"Forecast_Change\"})\n",
    "\n",
    "            result2[\"Forecast_Data_Level\"]= data_level\n",
    "            result2[\"Forecast_Data_Level_Key\"] = result2[\"Forecast_Data_Level_Key\"].str.strip(\"_\")\n",
    "            result2[\"Forecast_Method\"] = \"Model 2\"\n",
    "            result2[\"Forecast_Method_Version\"] = \"V1\"\n",
    "            result2[\"Forecasted_Flag\"] = \"Y\"\n",
    "            result2[\"Forcast_Explain\"] = \"\"\n",
    "            result2[\"Actual_Values\"] = result2[\"Actual_Values\"].astype(np.float64)\n",
    "            # result2[\"Forecast_Lower\"] = 0.0\n",
    "            # result2[\"Forecast_Upper\"] = 0.0\n",
    "\n",
    "            result2[\"Forecast_Change\"] = 0.0\n",
    "            # result2[\"Forecast_Change\"].fillna(value=0, inplace=True)\n",
    "            # result2[\"Forecast_Change\"] = pd.to_numeric(result2[\"Forecast_Change\"], errors=\"coerce\")\n",
    "            # result2[\"Forecast_Change\"] = np.where(result2[\"Forecast_Change\"] == float(\"-inf\"),-1000.00,result2[\"Forecast_Change\"])\n",
    "            # result2[\"Forecast_Change\"] = np.where(result2[\"Forecast_Change\"] == float(\"inf\"),1000.00,result2[\"Forecast_Change\"])\n",
    "            # result2[\"Forecast_Change\"] = np.where(result2[\"Forecast_Change\"] > 1000,1000.00,result2[\"Forecast_Change\"])  \n",
    "            # result2[\"Forecast_Change\"] = np.where(result2[\"Forecast_Change\"] < -1000,-1000.00,result2[\"Forecast_Change\"])\n",
    "            # # result2[\"Forecast_Change\"] = np.clip(result2[\"Forecast_Change\"], -1000.00, 1000.00)\n",
    "\n",
    "\n",
    "            result2[\"Forecasted_Value\"] = np.where(result2[\"Forecasted_Value\"]<0,0,result2[\"Forecasted_Value\"])      \n",
    "            result2[\"Actual_Values\"] = result2[\"Actual_Values\"].astype(np.float64)\n",
    "            result2 = result2[[\"Forecast_Data_Level\",\"Forecast_Data_Level_Key\",\"Forecast_Method\", \"Forecast_Method_Version\",\"Forecasted_Flag\",\n",
    "                           \"Forecast_Date\",\"Actual_Values\", \"Forecasted_Value\", \"Forecast_Lower\", \"Forecast_Upper\",\"Forecast_Trend\",\n",
    "                           \"Forecast_heartbeat\",\"Forecast_Change\", \"Forcast_Explain\"]]\n",
    "            \n",
    "            ctx.log.info(\"Model 2 Forecasting Done\")\n",
    "            ctx.waitUntilDatasetActive(datasetIds=[dataset_id], maxRetries=4, maxWaitTime=15,incrementMultiplier=3)\n",
    "            ds = Dataset(dataset_id)  \n",
    "            ctx.log.info(\"Dataset Loaded\")\n",
    "            response = ds.smartAnalytics.create(table_name,result2,False,False,True)\n",
    "            if \"status\" in response and response[\"status\"] == \"success\":\n",
    "                ctx.log.info(\"smart analytics creation is done -> {}\".format(dt.datetime.utcnow()))\n",
    "                ctx.log.info(\"Creation of table {} was done properly and smart analytics is created successfully\".format(table_name))\n",
    "                try:\n",
    "                    ds.smartAnalytics.delete(\"reRunForecastDummy\")\n",
    "                    ctx.log.info(\"Successfully deleted reRunForecastDummy Table\")\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                ctx.log.error(\"smart analytics creation is failed -> {}\".format(dt.datetime.utcnow()))\n",
    "                return {\"status\": \"failed\", \"message\": \"Organization processing failed due to an error {}\".format(response[\"message\"] if \"message\" in response else \"internalServerError\")}\n",
    "\n",
    "\n",
    "        if \"Model_3\" in select_model or \"model_3\" in select_model:\n",
    "            ctx.log.info(\"Model 3 Forecasting Started\")\n",
    "            ds = Dataset(dataset_id)\n",
    "            f = ds.sqlDf(f\"select * from #{table_name}\")\n",
    "            columns = f.columns.str.strip(\"m_\")\n",
    "            f.columns = columns\n",
    "\n",
    "            m1 = f[f[\"Forecast_Method\"] == \"Model 1\"]\n",
    "            result = f[f[\"Forecast_Method\"] == \"Model 2\"]\n",
    "\n",
    "            m1.columns = columns\n",
    "            result.columns = columns\n",
    "\n",
    "            m1 = m1[m1[\"Forecast_Data_Level\"] == data_level]\n",
    "            result = result[result[\"Forecast_Data_Level\"] == data_level]\n",
    "\n",
    "            result = result.sort_values([\"Forecast_Data_Level_Key\",\"Forecast_Date\"], ignore_index=True)\n",
    "            m1 = m1.sort_values([\"Forecast_Data_Level_Key\",\"Forecast_Date\"], ignore_index=True)\n",
    "\n",
    "            actuals = m1[\"Actual_Values\"].tolist()\n",
    "            dates = m1[\"Forecast_Date\"].tolist()\n",
    "            key = m1[\"Forecast_Data_Level_Key\"].tolist()\n",
    "            m3 = pd.DataFrame()\n",
    "            fm3 = []\n",
    "\n",
    "            for x,y in zip(m1[\"Forecasted_Value\"],result[\"Forecasted_Value\"]):\n",
    "                z = (x+y)/2\n",
    "                fm3.append(z)\n",
    "\n",
    "            m3[\"fcst\"] = fm3\n",
    "            m3[\"Forecast_Data_Level_Key\"] = key\n",
    "            m3[\"Forecast_Data_Level\"] = data_level\n",
    "            m3[\"Forecast_Method\"] = \"Model 3\"\n",
    "            m3[\"Forecast_Method_Version\"] = \"V1\"\n",
    "            m3[\"Forecasted_Flag\"] = \"Y\"\n",
    "            m3[\"Forecast_Date\"] = dates\n",
    "            m3[\"Actual_Values\"] = actuals\n",
    "            m3[\"Forecast_Lower\"] = 0.0\n",
    "            m3[\"Forecast_Upper\"] = 0.0\n",
    "            m3[\"Forcast_Explain\"] = \"\"\n",
    "            m3[\"fcst\"] = m3[\"fcst\"].round()\n",
    "\n",
    "            prod_ids = m3[\"Forecast_Data_Level_Key\"].unique()  \n",
    "            frames_of_data = pd.DataFrame()\n",
    "            for i in prod_ids:\n",
    "                data_frame = m3[m3[\"Forecast_Data_Level_Key\"] == i]\n",
    "                data_frame = data_frame.sort_values([\"Forecast_Date\"]).reset_index(drop=True)\n",
    "                data_frame[\"Forecast_Trend\"] = TrendCal(data_frame)\n",
    "                data_frame[\"Forecast_heartbeat\"] = Heartbeat(data_frame)\n",
    "                data_frame[\"Forecast_Change\"] = Percentage_change(data_frame)\n",
    "                frames_of_data = pd.concat([frames_of_data,data_frame])\n",
    "\n",
    "\n",
    "            frames_of_data[\"Forecast_Change\"] = 0.0\n",
    "            # frames_of_data[\"Forecast_Change\"].fillna(value=0, inplace=True)\n",
    "            # frames_of_data[\"Forecast_Change\"] = pd.to_numeric(frames_of_data[\"Forecast_Change\"], errors=\"coerce\")\n",
    "            # frames_of_data[\"Forecast_Change\"] = np.where(frames_of_data[\"Forecast_Change\"] == float(\"-inf\"),-1000.00,frames_of_data[\"Forecast_Change\"])\n",
    "            # frames_of_data[\"Forecast_Change\"] = np.where(frames_of_data[\"Forecast_Change\"] == float(\"inf\"),1000.00,frames_of_data[\"Forecast_Change\"])\n",
    "            # frames_of_data[\"Forecast_Change\"] = np.where(frames_of_data[\"Forecast_Change\"] > 1000,1000.00,frames_of_data[\"Forecast_Change\"])  \n",
    "            # frames_of_data[\"Forecast_Change\"] = np.where(frames_of_data[\"Forecast_Change\"]< -1000,-1000.00,frames_of_data[\"Forecast_Change\"]) \n",
    "\n",
    "            frames_of_data=frames_of_data[[\"Forecast_Data_Level\",\"Forecast_Data_Level_Key\",\"Forecast_Method\", \"Forecast_Method_Version\",\"Forecasted_Flag\",\n",
    "                           \"Forecast_Date\",\"Actual_Values\", \"fcst\", \"Forecast_Lower\", \"Forecast_Upper\",\"Forecast_Trend\",\n",
    "                           \"Forecast_heartbeat\",\"Forecast_Change\", \"Forcast_Explain\"]]\n",
    "\n",
    "            frames_of_data.columns = [\"Forecast_Data_Level\",\"Forecast_Data_Level_Key\",\"Forecast_Method\", \"Forecast_Method_Version\",\"Forecasted_Flag\",\n",
    "                               \"Forecast_Date\",\"Actual_Values\", \"Forecasted_Value\", \"Forecast_Lower\", \"Forecast_Upper\",\"Forecast_Trend\",\n",
    "                               \"Forecast_heartbeat\",\"Forecast_Change\", \"Forcast_Explain\"]\n",
    "            ctx.log.info(\"Model 3 Forecasting Done\")\n",
    "            ctx.waitUntilDatasetActive(datasetIds=[dataset_id], maxRetries=4, maxWaitTime=15,incrementMultiplier=3)\n",
    "            ds = Dataset(dataset_id)\n",
    "            ctx.log.info(\"Dataset Loaded\")\n",
    "            response = ds.smartAnalytics.create(table_name,frames_of_data,False,False,True)\n",
    "            if \"status\" in response and response[\"status\"] == \"success\":\n",
    "                ctx.log.info(\"smart analytics creation is done -> {}\".format(dt.datetime.utcnow()))\n",
    "                ctx.log.info(\"Creation of table {} was done properly and smart analytics is created successfully\".format(table_name))\n",
    "            else:\n",
    "                ctx.log.error(\"smart analytics creation is failed -> {}\".format(dt.datetime.utcnow()))\n",
    "                return {\"status\": \"failed\", \"message\": \"Organization processing failed due to an error {}\".format(response[\"message\"] if \"message\" in response else \"internalServerError\")}\n",
    "\n",
    "        return {\"status\": \"success\", \"message\": \"Forecasting Done Successfully\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        #print(traceback.format_exc())\n",
    "        return {\"status\": \"Failed\", \"message\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc20f48c-f115-42d4-b74a-626807bf620f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;34m[2024-03-22 07:52:22,988] [INFO] Forecasting_day has been successfully registered. The most recent version available is 0.3 !!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Forecasting_day.register(libraryName = \"t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6337bf85-5559-4fee-9403-d657d5aaefd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks loaded  !!\n"
     ]
    }
   ],
   "source": [
    "tsk.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbf07dff-56ab-4d2c-b679-64e235a55e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 0.3 for the task Forecasting_day has been successfully promoted to O  !!\n",
      "Tasks loaded  !!\n"
     ]
    }
   ],
   "source": [
    "tsk.t1.Forecasting_day.promote(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsk.t1.Forecasting_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37bde9c8-9a7e-4797-a42c-87b18de01275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flows loaded !!\n"
     ]
    }
   ],
   "source": [
    "flw = FlowLibrary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bccee62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Flow(name=\"csf_Forecast_d\", description= \"to do Forecast\") as flow:\n",
    "    query = \"\"\"\n",
    "    SELECT cast(m_actual_date as date) as \"actual_date\",\n",
    "    m_sales_quantity as \"sales_quantity\", m_product_num as \"product_num\"\n",
    "    FROM #FRCST_TEST\n",
    "    --where cast(m_actual_date as date) < \"2024-01-20\"\n",
    "    \"\"\"\n",
    "    ds_id = \"662ba9e1-BZoSk0BIm\"\n",
    "    idp = Parameter(\"Dataset Id\", ds_id)\n",
    "    q = Parameter(\"Sql Query\", query)\n",
    "    k = Parameter(\"\"\"Name of key column (the column that needs to forecasted like product id, It can also\n",
    "                  be multi-level like warehouse and then product id)\"\"\",\"product_num\")\n",
    "    period = Parameter(\"Period (The time duration in days/weeks/months to do Forecast)\",12)\n",
    "    allow_for = Parameter(\"Allow Force Forecast\", False)\n",
    "    d = Parameter(\"Name of Date Column\", \"actual_date\")\n",
    "    v = Parameter(\"Name of Metric Column\", \"sales_quantity\")\n",
    "    key_name = Parameter(\"Name of Forecast Type Level\", \"ProductLevel Monthly\")\n",
    "    tabel_name = Parameter(\"Name of Forecast Table\", \"Latest_Forecast\")\n",
    "    AutoArimaTable = Parameter(\"Name of AutoArima Parameter Table\", \"AutoArima_Param\")\n",
    "    o = Parameter(\"Overwrite\", True)\n",
    "    select_model = Parameter(\"Forecast Models\", \"Model_1,Model_2,Model_3\")\n",
    "    F_Mode = Parameter(\"Define Forecast Mode : day/week/month\", \"month\")\n",
    "    im = Parameter(\"Outlier Imputation Method : mean/median/mode/quartile value(qv=0.95)\", \"median\")\n",
    "    ot = Parameter(\"Outlier Threshold\", \"1.5\")\n",
    "    ofp = Parameter(\"Outlier Imputation Flag (set yes if Product Level)(if yes then put global flag be false)\", False)\n",
    "    gs = Parameter(\"\"\"Global Imputation Smoothing (set yes if you have atleast one off product with very high values)\n",
    "                    (if yes then put outlier flag be false)\"\"\", False)\n",
    "    reRun = Parameter(\"reRun the Flow\", False)\n",
    "    reRunModel = Parameter(\"Select Models From Where reRun Will Start: Model_1/Model_2 if reRun enabled else NA\", \"NA\")\n",
    "    \n",
    "    \n",
    "    a = tsk.t1.get_df(q,idp)\n",
    "    get_para = tsk.t1.get_parameter(a,k,d,v,period,F_Mode,ofp,im,ot,gs)\n",
    "    fore = tsk.t1.Forecasting_day(get_para,allow_for,idp,key_name,tabel_name,AutoArimaTable,o,select_model,F_Mode,reRun,reRunModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e96f7d72-bca6-4d9f-bb4f-ff94bc09e3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The flow csf_Forecast_test has been registered successfully. Latest version available now is 0.1 !!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.register(libraryName = \"Forecasting_v1\",flowName =\"csf_Forecast_d\",description = \"to do Forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5486f8ed-6be7-4b51-9a8b-d65b79213265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flows loaded !!\n"
     ]
    }
   ],
   "source": [
    "flw.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbddae7a-59d9-442c-953e-92a79b13b102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Version 0.1 for the flow csf_Forecast_test has been promoted to O successfully !!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flw.Forecasting_v1.csf_Forecast_d.promote(\"P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d241853-f904-4653-86e3-dff76cdfb8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 0.1 has been deleted.\n",
      "Flows loaded !!\n"
     ]
    }
   ],
   "source": [
    "#flw.Forecasting_v1.csf_Forecast_d.deleteVersion(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4296f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feada189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your dataframe with columns \"item_no\", \"date\", \"value\"\n",
    "\n",
    "# Example dataframe creation\n",
    "data = {\n",
    "    \"item_no\": [1, 1, 1, 2, 2,2,2,2,2,2,2, 3, 3, 3, 3, 4, 4],\n",
    "    \"date\": pd.date_range(\"2024-01-01\", periods=17, freq=\"D\"),\n",
    "    \"value\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 0 ,1,1,1,1,1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Group by \"item_no\", count rows, and filter out items with less than 6 rows\n",
    "counts = df.groupby(\"item_no\").size()\n",
    "valid_items = counts[counts >= 6].index\n",
    "\n",
    "# Filter the original dataframe based on valid item numbers\n",
    "filtered_df = df[df[\"item_no\"].isin(valid_items)]\n",
    "\n",
    "# Output the filtered dataframe\n",
    "filtered_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
