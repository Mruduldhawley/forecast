{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b34d5d-fbd1-4f0a-b32b-9bc09c88ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d325b5-930d-4722-be47-6e70d0a6c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import datetime as dt\n",
    "from conversight import Dataset, Flow, Parameter, SmartAnalytics,task, ProactiveInsights,TaskLibrary, FlowLibrary, Context\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bfb2c06-e058-433b-9db2-0a144789b56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks loaded  !!\n"
     ]
    }
   ],
   "source": [
    "tsk = TaskLibrary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "560b5068-96d2-4887-a029-956f9c36228f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><head><style>\n",
       "            #table_css {\n",
       "            font-family: Arial, Helvetica, sans-serif;\n",
       "            border-collapse: collapse;\n",
       "            width: 99.8%;\n",
       "            table-layout: auto;\n",
       "            font-size: 14px\n",
       "            }\n",
       "            #table_css td, #table_css th {\n",
       "            border: 1px solid #ddd;\n",
       "            padding: 8px;\n",
       "            text-align:center;\n",
       "            width:auto;\n",
       "            }\n",
       "            #table_css tr {background-color: #f2f2f2;}\n",
       "            #table_css tr:hover {background-color: #ddd;}\n",
       "            #table_css th {\n",
       "            padding-top: 12px;\n",
       "            padding-bottom: 12px;\n",
       "            text-align: center;\n",
       "            background-color: #04AA6D;\n",
       "            color: white;\n",
       "            }\n",
       "            .left-align {text-align:left !important;}\n",
       "    </style></head><body><table id='table_css'><tr> <th>#</th> <th>Version</th> <th>Description</th> <th>Type</th> <th style='width: 62px;'>Sub Type</th> <th>Level</th> <th>Inputs</th> <th>Outputs</th> <th style='width: 110px;'>Registered ORG</th> <th>Slug</th> </tr> <tr> <td>1 </td> <td> 0.2 </td> <td class='left-align'> Task description goes here. </td> <td> generic </td>  <td> Function </td> <td> O </td> <td> query: str, ds_id: str </td> <td> Attribute </td> <td> Aev-Conversions </td> <td> 5014fbdc-e065-4ca4-8763-7a9f86be9d3e </td></tr> </table></body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsk.t1.get_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed0cc9e-4a51-4221-ba78-78a73a8c6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csContext import CSContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a749996-869a-40a3-b46e-601821b19ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def frcst_preprocessing(ctx: CSContext, df: pd.core.frame.DataFrame, key_col: str, dat_col: str, mat_col: str, F_Mode: str,\n",
    "                  outlier_flag_by_product: bool, imputation_method: str, outlier_threshold: str, global_smoothing: bool)->dict:\n",
    "    \"\"\"\n",
    "    Write the doc string here, Refer Forecasting Model task\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import datetime as dt\n",
    "        from conversight import Dataset,Context\n",
    "        # from conversight import TaskPool\n",
    "        # tp = TaskPool(pool_size=480)\n",
    "        # ctx=Context()\n",
    "        ctx.log.info(\"preprocessing is started ->\")\n",
    "        ctx.log.info(f\"selected forecast mode : {F_Mode}\")\n",
    "        outlier_threshold = float(outlier_threshold)\n",
    "    \n",
    "        def impute_outliers_by_product(df, key_column, value_column, imputation_value, threshold):\n",
    "            ctx.log.info(f\"Outliers deduction by Product method {imputation_value} with threshold {threshold}\")\n",
    "            def impute_outliers_group(group):\n",
    "                group[value_column] = group[value_column].apply(lambda x: max(0, x))\n",
    "                Q1 = group[value_column].quantile(0.25)\n",
    "                Q3 = group[value_column].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - threshold * IQR\n",
    "                upper_bound = Q3 + threshold * IQR\n",
    "                outliers_mask = (group[value_column] < lower_bound) | (group[value_column] > upper_bound)\n",
    "                if imputation_value.lower() == \"mean\":\n",
    "                    imputed_value = group[value_column].mean()\n",
    "                elif imputation_value.lower() == \"median\":\n",
    "                    imputed_value = group[value_column].median()\n",
    "                elif imputation_value.lower() == \"mode\":\n",
    "                    imputed_value = group[value_column].mode().iloc[0]\n",
    "                elif \"qv\" in imputation_value.lower():\n",
    "                    imputation_value_qv = imputation_value.split(\"=\")[1]\n",
    "                    qv = float(imputation_value_qv)\n",
    "                    qv = group[value_column].quantile(qv)\n",
    "                    imputed_value = qv\n",
    "                else:\n",
    "                    iv = float(imputation_value)\n",
    "                    imputed_value = iv\n",
    "                group[value_column] = group[value_column].mask(outliers_mask, imputed_value)\n",
    "                return group\n",
    "            ctx.log.info(\"Identifying and imputing outliers..\")\n",
    "            df = df.groupby(key_column, observed=True).apply(impute_outliers_group)\n",
    "            return df\n",
    "\n",
    "        def impute_outliers(df, column_name, imputation_value, threshold):\n",
    "            ctx.log.info(f\"Outliers deduction method {imputation_value} with threshold {threshold}\")\n",
    "            df[column_name] = df[column_name].apply(lambda x: max(0, x))\n",
    "            Q1 = df[column_name].quantile(0.25)\n",
    "            Q3 = df[column_name].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]\n",
    "            if imputation_value.lower() == \"mean\":\n",
    "                imputed_value = df[column_name].mean()\n",
    "            elif imputation_value.lower() == \"median\":\n",
    "                imputed_value = df[column_name].median()\n",
    "            elif imputation_value.lower() == \"mode\":\n",
    "                imputed_value = df[column_name].mode().iloc[0]\n",
    "            elif \"qv\" in imputation_value.lower():\n",
    "                imputation_value_qv = imputation_value.split(\"=\")[1]\n",
    "                qv = float(imputation_value_qv)\n",
    "                qv = df[column_name].quantile(qv)\n",
    "                imputed_value = qv\n",
    "            else:\n",
    "                iv = float(imputation_value)\n",
    "                imputed_value = iv\n",
    "            ctx.log.info(\"Identifying and imputing outliers..\")\n",
    "            df[column_name] = df[column_name].mask((df[column_name] < lower_bound) | (df[column_name] > upper_bound), imputed_value)\n",
    "            return df\n",
    "\n",
    "        def missed_data(df,keys,dat_col,mat_col,i,min_date,max_date,F_Mode):\n",
    "            df1 = df[df[keys]==i].reset_index(drop=True)\n",
    "            df1[dat_col] = pd.to_datetime(df1[dat_col]).dt.floor(\"D\")\n",
    "            df1 = df1.sort_values([dat_col])\n",
    "            training = df1.groupby(dat_col, observed=True)[mat_col].sum().reset_index()\n",
    "            if F_Mode.lower() == \"week\":\n",
    "                dates_required = pd.date_range(min_date, max_date, freq=\"W-MON\", inclusive=\"left\").strftime(\"%Y-%m-%d\").tolist()\n",
    "            elif F_Mode.lower() == \"month\":\n",
    "                dates_required = pd.date_range(min_date, max_date, freq=\"MS\").strftime(\"%Y-%m-01\").tolist()\n",
    "            dates_required = pd.DataFrame(dates_required, columns = [dat_col])\n",
    "            dates_required[dat_col] = pd.to_datetime(dates_required[dat_col]).dt.floor(\"D\")\n",
    "            processed_data = dates_required.merge(training, on=dat_col, how=\"left\")\n",
    "            processed_data.columns = [dat_col, mat_col]\n",
    "            processed_data = processed_data.fillna(0)\n",
    "            processed_data[keys] = i\n",
    "            processed_data[dat_col] = pd.to_datetime(processed_data[dat_col])\n",
    "            return processed_data\n",
    "\n",
    "        def missed_data_faster(df, keys, dat_col, mat_col, min_date, max_date, F_Mode):\n",
    "            df[dat_col] = pd.to_datetime(df[dat_col]).dt.floor(\"D\")\n",
    "            if F_Mode.lower() == \"month\":\n",
    "                #all_dates = pd.date_range(min_date, max_date, freq=\"MS\", inclusive=\"left\")\n",
    "                all_dates = pd.date_range(min_date, max_date, freq=\"MS\", inclusive=\"both\")\n",
    "            elif F_Mode.lower() == \"week\":\n",
    "                all_dates = pd.date_range(min_date, max_date, freq=\"W-MON\", inclusive=\"both\")\n",
    "            #all_dates_df = pd.DataFrame({dat_col: all_dates})\n",
    "            df.set_index([keys, dat_col], inplace=True)\n",
    "            df_unstacked = df.unstack(level=0).reindex(all_dates).stack(dropna=False).reset_index()\n",
    "            df_unstacked[mat_col].fillna(0, inplace=True)\n",
    "            df_unstacked.rename(columns={'level_0': dat_col}, inplace=True)\n",
    "            return df_unstacked\n",
    "            \n",
    "        def missed_data_optimized(df, keys, dat_col, mat_col, min_date, max_date, F_Mode):\n",
    "            df[dat_col] = pd.to_datetime(df[dat_col]).dt.floor(\"D\")\n",
    "            if F_Mode.lower() == \"month\":\n",
    "                dates_required = pd.date_range(min_date, max_date, freq=\"MS\").strftime(\"%Y-%m-01\").tolist()\n",
    "            elif F_Mode.lower() == \"week\":\n",
    "                dates_required = pd.date_range(min_date, max_date, freq=\"W-MON\").strftime(\"%Y-%m-%d\").tolist()\n",
    "            dates_required = pd.DataFrame(dates_required, columns=[dat_col])\n",
    "            dates_required[dat_col] = pd.to_datetime(dates_required[dat_col]).dt.floor(\"D\")\n",
    "            unique_keys = df[keys].unique()\n",
    "            all_combinations = pd.MultiIndex.from_product([unique_keys, dates_required[dat_col]], names=[keys, dat_col]).to_frame(index=False)\n",
    "            grouped = df.groupby([keys, dat_col], observed=True)[mat_col].sum().reset_index()\n",
    "            processed_data = all_combinations.merge(grouped, on=[keys, dat_col], how=\"left\")\n",
    "            processed_data[mat_col] = processed_data[mat_col].fillna(0)\n",
    "            return processed_data\n",
    "        \n",
    "        #### Merging all key column and drop null values for the main columns\n",
    "        columns = key_col.split(\",\")\n",
    "        keys = \"key_column\"\n",
    "        df[keys] = df[columns].astype(str).agg(\"_\".join, axis=1)\n",
    "        columns.append(keys)\n",
    "        columns.extend([dat_col, mat_col])\n",
    "        df = df.dropna(subset=columns).reset_index(drop=True)\n",
    "\n",
    "        #### Drop duplicate\n",
    "        df = df.drop_duplicates()\n",
    "    \n",
    "        #### select necessary columns and Convert date to month start or monday start week date\n",
    "        df = df[[dat_col, mat_col, keys]]\n",
    "        df[keys] = df[keys].astype(str)\n",
    "        df[dat_col] = pd.to_datetime(df[dat_col]).dt.floor(\"D\")\n",
    "        if F_Mode.lower() == \"month\":\n",
    "            df[dat_col] = ((df[dat_col] + pd.offsets.MonthEnd(0) - pd.offsets.MonthBegin(1)).dt.floor(\"D\"))\n",
    "        elif F_Mode.lower() == \"week\":\n",
    "            df[dat_col] = df[dat_col].dt.to_period(\"W\").apply(lambda r: r.start_time)\n",
    "        df = df.groupby([dat_col, keys], observed=True)[mat_col].sum().reset_index()\n",
    "\n",
    "        #### Remove Current month/week data from training\n",
    "        if F_Mode.lower() == \"month\":\n",
    "          P = dt.datetime.today().replace(day=1).strftime(\"%Y-%m-%d\")\n",
    "          df = df[df[dat_col] < P].reset_index(drop=True)\n",
    "        elif F_Mode.lower() == \"week\":\n",
    "          #current week start monday\n",
    "          current_date = dt.datetime.today()\n",
    "          P = (current_date - dt.timedelta(days=current_date.weekday())).strftime(\"%Y-%m-%d\")\n",
    "          df = df[df[dat_col] < P].reset_index(drop=True)\n",
    "        \n",
    "        #### remove nagetive values\n",
    "        df[mat_col] = df[mat_col].apply(lambda x: max(0, x))\n",
    "\n",
    "        #### Remove outliers from the training data\n",
    "        if outlier_flag_by_product and global_smoothing:\n",
    "            ctx.log.error(\"Both variables are True, set according...\")\n",
    "            return {\"status\": \"Failed\", \"message\": \"processing failed due to bad configuration\"}\n",
    "        else:\n",
    "            if outlier_flag_by_product:\n",
    "                ctx.log.info(\"outlier_flag_by_product is started ->\")\n",
    "                df = impute_outliers_by_product(df, keys, mat_col, imputation_method, outlier_threshold)\n",
    "                ctx.log.info(\"outlier deduction is ended ->\")\n",
    "            if global_smoothing:\n",
    "                ctx.log.info(\"global_smoothing is started ->\")\n",
    "                df = impute_outliers(df, mat_col, imputation_method, outlier_threshold)\n",
    "                ctx.log.info(\"outlier deduction is ended ->\")\n",
    "    \n",
    "        # Find maximum date and Minimum date\n",
    "        min_date_limit, max_date_limit = df[dat_col].min(), df[dat_col].max()\n",
    "        ctx.log.info(\"Min_date_limit and Max_date_limit calculated successfully\")\n",
    "        ctx.log.info(f\"Min - Date --> {min_date_limit} and Max - Date --> {max_date_limit}\")\n",
    "        \n",
    "        #### Find the missing month/week date and fill with value column by zero\n",
    "        prod_ids = df[keys].unique()\n",
    "        ctx.log.info(\"Getting All unique Materials to do Forecast and Total unique count {}\".format(len(prod_ids)))\n",
    "        # frame, cnt = [], 1\n",
    "        # for i in  prod_ids:\n",
    "        #     #df1 = missed_data(df,keys,dat_col,mat_col,i,min_date_limit,max_date_limit,F_Mode)\n",
    "        #     tp.execute([df,keys,dat_col,mat_col,i,min_date_limit,max_date_limit,F_Mode], missed_data)\n",
    "        #     if cnt % 100 == 0:\n",
    "        #         frame.extend(tp.collect_all()[1])\n",
    "        #     if cnt%500 == 0:\n",
    "        #         ctx.log.info(\"Completed {} Items\".format(cnt))\n",
    "        #     cnt+=1\n",
    "        # # Collect all results after the loop has finished\n",
    "        # remaining_results = tp.collect_all()[1]\n",
    "        # frame.extend(remaining_results)\n",
    "        # result = pd.concat(frame)\n",
    "        # result = result.reset_index(drop=True)\n",
    "        result = missed_data_optimized(df, keys, dat_col, mat_col, min_date_limit, max_date_limit, F_Mode)\n",
    "        result[mat_col]=result[mat_col].apply(lambda x:round(x,2))\n",
    "    \n",
    "        ctx.log.info(\"preprocessing is ended ->\")\n",
    "        parameter = {\"dateID\":dat_col, \"keyID\":keys, \"matricID\":mat_col, \"DATA\":result, \"F_Mode\":F_Mode}\n",
    "        return parameter\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"Failed\", \"message\": str(e)}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84a370d-a3c5-4256-b70f-9202ac4a588f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;34m[2024-07-01 13:15:11,886] [INFO] frcst_preprocessing has been successfully registered. The most recent version available is 0.4 !!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "frcst_preprocessing.register(libraryName = \"t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e29f14d1-c8c6-45b9-8892-141a57444a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks loaded  !!\n"
     ]
    }
   ],
   "source": [
    "tsk.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bd32657-825a-4df9-8ea5-50e8c7971a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 0.4 for the task frcst_preprocessing has been successfully promoted to O  !!\n",
      "Tasks loaded  !!\n"
     ]
    }
   ],
   "source": [
    "tsk.t1.frcst_preprocessing.promote(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e2993e3-5b54-42a7-a398-df33a021959a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><head><style>\n",
       "            #table_css {\n",
       "            font-family: Arial, Helvetica, sans-serif;\n",
       "            border-collapse: collapse;\n",
       "            width: 99.8%;\n",
       "            table-layout: auto;\n",
       "            font-size: 14px\n",
       "            }\n",
       "            #table_css td, #table_css th {\n",
       "            border: 1px solid #ddd;\n",
       "            padding: 8px;\n",
       "            text-align:center;\n",
       "            width:auto;\n",
       "            }\n",
       "            #table_css tr {background-color: #f2f2f2;}\n",
       "            #table_css tr:hover {background-color: #ddd;}\n",
       "            #table_css th {\n",
       "            padding-top: 12px;\n",
       "            padding-bottom: 12px;\n",
       "            text-align: center;\n",
       "            background-color: #04AA6D;\n",
       "            color: white;\n",
       "            }\n",
       "            .left-align {text-align:left !important;}\n",
       "    </style></head><body><table id='table_css'><tr> <th>#</th> <th>Version</th> <th>Description</th> <th>Type</th> <th style='width: 62px;'>Sub Type</th> <th>Level</th> <th>Inputs</th> <th>Outputs</th> <th style='width: 110px;'>Registered ORG</th> <th>Slug</th> </tr> <tr> <td>1 </td> <td> 0.1 </td> <td class='left-align'> Task description goes here. </td> <td> generic </td>  <td> Function </td> <td> O </td> <td> df: pd.core.frame.DataFrame, key_col: str, dat_col: str, mat_col: str, F_Mode: str, outlier_flag_by_product: bool, imputation_method: str, outlier_threshold: str, global_smoothing: bool </td> <td> Name </td> <td> Aev-Conversions </td> <td> b5f0aa10-808d-42c4-97a8-8625602689fd </td></tr><tr> <td>2 </td> <td> 0.2 </td> <td class='left-align'> Task description goes here. </td> <td> generic </td>  <td> Function </td> <td> O </td> <td> df: pd.core.frame.DataFrame, key_col: str, dat_col: str, mat_col: str, F_Mode: str, outlier_flag_by_product: bool, imputation_method: str, outlier_threshold: str, global_smoothing: bool </td> <td> Name </td> <td> Aev-Conversions </td> <td> b5f0aa10-808d-42c4-97a8-8625602689fd </td></tr><tr> <td>3 </td> <td> 0.3 </td> <td class='left-align'> Task description goes here. </td> <td> generic </td>  <td> Function </td> <td> O </td> <td> df: pd.core.frame.DataFrame, key_col: str, dat_col: str, mat_col: str, F_Mode: str, outlier_flag_by_product: bool, imputation_method: str, outlier_threshold: str, global_smoothing: bool </td> <td> Name </td> <td> Aev-Conversions </td> <td> b5f0aa10-808d-42c4-97a8-8625602689fd </td></tr><tr> <td>4 </td> <td> 0.4 </td> <td class='left-align'> Task description goes here. </td> <td> generic </td>  <td> Function </td> <td> O </td> <td> ctx: CSContext, df: pd.core.frame.DataFrame, key_col: str, dat_col: str, mat_col: str, F_Mode: str, outlier_flag_by_product: bool, imputation_method: str, outlier_threshold: str, global_smoothing: bool </td> <td> Name </td> <td> Aev-Conversions </td> <td> b5f0aa10-808d-42c4-97a8-8625602689fd </td></tr> </table></body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsk.t1.frcst_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1501696f-b902-4a99-80a7-86d1c7bac061",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def autoArimaForecastParams(ctx: CSContext, p: dict, dataset_id:str, table_name:str):\n",
    "    \"\"\"\n",
    "    Write the doc string here, Refer Forecasting Model task\n",
    "    Final result dataframe column names are: key_column,p,d,q,sP,sD,sQ,m and seasonal\n",
    "    Final table name : AutoArima_Param\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # import library\n",
    "        import os\n",
    "        import sys\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        import pandas as pd\n",
    "        import datetime as dt\n",
    "        from conversight import Dataset,Context\n",
    "        from conversight import TaskPool\n",
    "        tp = TaskPool(pool_size=480)\n",
    "        # ctx=Context()\n",
    "        ctx.log.info(\"autoArima Model Forecast checking is started ->\")\n",
    "        \n",
    "        # do necesaary part for training\n",
    "        dat_col, mat_col, key_col = p[\"dateID\"], p[\"matricID\"], p[\"keyID\"]\n",
    "        fieldMapping = {\"date_column\":dat_col, \"value_column\":mat_col}\n",
    "        df = p[\"DATA\"]\n",
    "        F_Mode = p[\"F_Mode\"]\n",
    "        \n",
    "        # training and validation *use suppress logs\n",
    "        class suppress_logs(object):\n",
    "            \"\"\"\n",
    "            A context manager for doing a \"deep suppression\" of stdout and stderr in Python, i.e. will suppress all print, even if the print originates\n",
    "            in a compiled C/Fortran sub-function. This will not suppress raised exceptions, since exceptions are printed to stderr just before a script exits,\n",
    "            and after the context manager has exited (at least, I think that is why it lets exceptions through).\n",
    "            \"\"\"\n",
    "            def __init__(self):\n",
    "                self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n",
    "                self.save_fds = [os.dup(1), os.dup(2)]\n",
    "            def __enter__(self):\n",
    "                os.dup2(self.null_fds[0], 1)\n",
    "                os.dup2(self.null_fds[1], 2)\n",
    "            def __exit__(self, *_):\n",
    "                os.dup2(self.save_fds[0], 1)\n",
    "                os.dup2(self.save_fds[1], 2)\n",
    "                for fd in self.null_fds + self.save_fds:\n",
    "                    os.close(fd)\n",
    "                    \n",
    "        def forecastmodel2(df, prod_id, keys, F_Mode):\n",
    "            try:\n",
    "                from pandas.tseries.offsets import DateOffset\n",
    "                from statsmodels.tsa.stattools import adfuller\n",
    "                import statsmodels.api as sm\n",
    "                from statsforecast import StatsForecast\n",
    "                from statsforecast.models import AutoARIMA\n",
    "                from statsforecast.arima import arima_string\n",
    "                from scipy.stats import kruskal\n",
    "\n",
    "                df_model = df[df[keys] == prod_id]\n",
    "                df_model.drop([keys],axis = 1, inplace = True)\n",
    "                df_model = df_model.reset_index(drop=True)\n",
    "\n",
    "                def seasonality_test(series, F_Mode):\n",
    "                    if F_Mode.lower() == \"month\":\n",
    "                        m, fq = 12, \"MS\"\n",
    "                    elif F_Mode.lower() == \"week\":\n",
    "                        m, fq = 52, \"W-MON\"\n",
    "                    idx = np.arange(len(series.index)) % m\n",
    "                    H_statistic, p_value = kruskal(series, idx)\n",
    "                    if p_value <= 0.05:\n",
    "                        seasonal = True\n",
    "                    else: seasonal = False\n",
    "                    return m, fq, seasonal\n",
    "                \n",
    "                def modeSeasonality(series, F_Mode):\n",
    "                    from scipy.stats import bartlett, levene\n",
    "                    if F_Mode.lower() == \"month\":\n",
    "                        sfq = 12\n",
    "                    elif F_Mode.lower() == \"week\":\n",
    "                        sfq = 52\n",
    "                    moving_average = series.rolling(window=sfq, center=True).mean()\n",
    "                    seasonal_ratios = series / moving_average\n",
    "                    seasonal_ratios = seasonal_ratios.dropna()\n",
    "                    group1 = seasonal_ratios[:len(seasonal_ratios)//2]\n",
    "                    group2 = seasonal_ratios[len(seasonal_ratios)//2:]\n",
    "                    \n",
    "                    # Perform Levene's test\n",
    "                    levene_stat, levene_p_value = levene(group1, group2)\n",
    "                    if levene_p_value > 0.05:\n",
    "                        smode = 'multiplicative'\n",
    "                    else:\n",
    "                        smode = 'additive'\n",
    "                    return smode\n",
    "                \n",
    "\n",
    "                df_model[\"unique_id\"] = 1\n",
    "                df_model.columns = [\"ds\",\"y\",\"unique_id\"]\n",
    "                sl, fq, seas = seasonality_test(df_model[\"y\"], F_Mode)\n",
    "                smode = modeSeasonality(df_model['y'], F_Mode)\n",
    "                sf = StatsForecast(df=df_model,\n",
    "                                models = [AutoARIMA(season_length = sl, seasonal=seas,\n",
    "                                                    start_p=0, start_q=0, start_P=0, start_Q=0,\n",
    "                                                    max_p= 5, max_q=5, max_P=3, max_Q=3,\n",
    "                                                    max_order=20, max_d=2, #max_D=2,\n",
    "                                                    # test=\"adf\", seasonal_test=\"ocsb\",\n",
    "                                                    )\n",
    "                                            ],freq = fq, n_jobs=-1\n",
    "                                )\n",
    "                with suppress_logs():\n",
    "                    sf.fit()\n",
    "                #print(arima_string(sf.fitted_[0,0].model_))\n",
    "                #print(sf.fitted_[0][0].model_[\"arma\"])\n",
    "                res = sf.fitted_[0][0].model_[\"arma\"]\n",
    "                p, q, P, Q, m, d, D = res[0], res[1], res[2], res[3], res[4], res[5], res[6]\n",
    "                if P==0 and Q==0 and D==0:\n",
    "                    m=0\n",
    "                data = {keys:[prod_id], \"p\":[p], \"d\":[d], \"q\":[q], \"sP\":[P], \"sD\":[D], \"sQ\":[Q], \"m\":[m],\n",
    "                        \"seasonal\":[seas],\"seasonalMode\":[smode]}\n",
    "                result = pd.DataFrame(data)\n",
    "                del sf, df_model\n",
    "                return result\n",
    "            except:\n",
    "                ctx.log.info(\"**AutoArima Paramets failed for Item: {}\".format(prod_id))\n",
    "\n",
    "        ##### Main forecast execution start #######################################################\n",
    "        prod_ids = df[key_col].unique()\n",
    "        ctx.log.info(\"Getting All unique Materials to do Forecast and Total unique count {}\".format(len(prod_ids)))\n",
    "        frame = []  \n",
    "        cnt = 1\n",
    "        for i in prod_ids:\n",
    "            #frame.append(forecastmodel2(df, i, key_col, F_Mode))\n",
    "            tp.execute([df, i, key_col, F_Mode], forecastmodel2)\n",
    "            if cnt % 100 == 0:\n",
    "                frame.extend(tp.collect_all()[1])\n",
    "            if cnt%500 == 0:\n",
    "                ctx.log.info(\"Completed {} Items\".format(cnt))\n",
    "            cnt+=1\n",
    "        # Collect all results after the loop has finished\n",
    "        remaining_results = tp.collect_all()[1]\n",
    "        frame.extend(remaining_results)\n",
    "        \n",
    "        result_df = pd.concat(frame)\n",
    "        result_df = result_df.reset_index(drop=True)\n",
    "                \n",
    "        # load back to db\n",
    "        ctx.log.info(\"autoArima Model Forecast Parameters checking Done\")\n",
    "        ctx.waitUntilDatasetActive(datasetIds=[dataset_id], maxRetries=4, maxWaitTime=15,incrementMultiplier=3)\n",
    "        ds = Dataset(dataset_id)\n",
    "        ctx.log.info(\"Dataset Loaded\")\n",
    "        response = ds.smartAnalytics.create(table_name, result_df, False, True, True)\n",
    "        if \"status\" in response and response[\"status\"] == \"success\":\n",
    "            ctx.log.info(\"smart analytics creation is done -> {}\".format(dt.datetime.utcnow()))\n",
    "            ctx.log.info(\"Creation of table {} was done properly and smart analytics is created successfully\".format(table_name))\n",
    "        else:\n",
    "            ctx.log.error(\"smart analytics creation is failed -> {}\".format(dt.datetime.utcnow()))\n",
    "            return {\"status\": \"failed\", \"message\": \"Organization processing failed due to an error {}\".format(response[\"message\"] if \"message\" in response else \"internalServerError\")}\n",
    "\n",
    "        return {\"status\": \"success\", \"message\": \"Forecasting Done Successfully\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"Failed\", \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aa91be2-9e29-48ca-a2fa-f660d0636161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;34m[2024-07-01 13:22:34,385] [INFO] autoArimaForecastParams has been successfully registered. The most recent version available is 0.4 !!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "autoArimaForecastParams.register(libraryName = \"t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12af77e0-3d5d-4515-b68e-71ddfece3b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks loaded  !!\n"
     ]
    }
   ],
   "source": [
    "tsk.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3d154bc-b133-4e9d-af21-0b679b2166e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 0.4 for the task autoArimaForecastParams has been successfully promoted to O  !!\n",
      "Tasks loaded  !!\n"
     ]
    }
   ],
   "source": [
    "tsk.t1.autoArimaForecastParams.promote(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c55ea57e-8acf-4210-87de-40fc02e2c1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><head><style>\n",
       "            #table_css {\n",
       "            font-family: Arial, Helvetica, sans-serif;\n",
       "            border-collapse: collapse;\n",
       "            width: 99.8%;\n",
       "            table-layout: auto;\n",
       "            font-size: 14px\n",
       "            }\n",
       "            #table_css td, #table_css th {\n",
       "            border: 1px solid #ddd;\n",
       "            padding: 8px;\n",
       "            text-align:center;\n",
       "            width:auto;\n",
       "            }\n",
       "            #table_css tr {background-color: #f2f2f2;}\n",
       "            #table_css tr:hover {background-color: #ddd;}\n",
       "            #table_css th {\n",
       "            padding-top: 12px;\n",
       "            padding-bottom: 12px;\n",
       "            text-align: center;\n",
       "            background-color: #04AA6D;\n",
       "            color: white;\n",
       "            }\n",
       "            .left-align {text-align:left !important;}\n",
       "    </style></head><body><table id='table_css'><tr> <th>#</th> <th>Version</th> <th>Description</th> <th>Type</th> <th style='width: 62px;'>Sub Type</th> <th>Level</th> <th>Inputs</th> <th>Outputs</th> <th style='width: 110px;'>Registered ORG</th> <th>Slug</th> </tr> <tr> <td>1 </td> <td> 0.1 </td> <td class='left-align'> Task description goes here. </td> <td> generic </td>  <td> Function </td> <td> O </td> <td> p: dict, dataset_id: str, table_name: str </td> <td> str </td> <td> Aev-Conversions </td> <td> af73efaa-77aa-4a3b-a23e-582ac12afa09 </td></tr><tr> <td>2 </td> <td> 0.2 </td> <td class='left-align'> Task description goes here. </td> <td> generic </td>  <td> Function </td> <td> O </td> <td> p: dict, dataset_id: str, table_name: str </td> <td> str </td> <td> Aev-Conversions </td> <td> af73efaa-77aa-4a3b-a23e-582ac12afa09 </td></tr><tr> <td>3 </td> <td> 0.3 </td> <td class='left-align'> Task description goes here. </td> <td> generic </td>  <td> Function </td> <td> O </td> <td> p: dict, dataset_id: str, table_name: str </td> <td> str </td> <td> Aev-Conversions </td> <td> af73efaa-77aa-4a3b-a23e-582ac12afa09 </td></tr><tr> <td>4 </td> <td> 0.4 </td> <td class='left-align'> Task description goes here. </td> <td> generic </td>  <td> Function </td> <td> O </td> <td> ctx: CSContext, p: dict, dataset_id: str, table_name: str </td> <td> str </td> <td> Aev-Conversions </td> <td> af73efaa-77aa-4a3b-a23e-582ac12afa09 </td></tr> </table></body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsk.t1.autoArimaForecastParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "010a4fca-2369-466c-88d0-1dbf1ff78486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flows loaded !!\n"
     ]
    }
   ],
   "source": [
    "flw = FlowLibrary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed431cf1-3c90-462d-b64f-bb69a9d1ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Flow(name=\"autoArimaForecastParams\", description= \"to do Forecast\") as flow:\n",
    "    query = \"\"\"\n",
    "    SELECT cast(m_actual_date as date) as \"actual_date\",\n",
    "    m_sales_quantity as \"sales_quantity\", m_product_num as \"product_num\"\n",
    "    FROM #FRCST_TEST\n",
    "    \"\"\"\n",
    "    ds_id = \"662ba9e1-BZoSk0BIm\"\n",
    "    idp = Parameter(\"Dataset Id\", ds_id)\n",
    "    q = Parameter(\"Sql Query\", query)\n",
    "    k = Parameter(\"\"\"Name of key column (the column that needs to forecasted like product id, It can also\n",
    "                  be multi-level like warehouse and then product id)\"\"\",\"product_num\")\n",
    "    d = Parameter(\"Name of Date Column\", \"actual_date\")\n",
    "    v = Parameter(\"Name of Metric Column\", \"sales_quantity\")\n",
    "    tabel_name = Parameter(\"Name of Forecast Table\", \"AutoArima_Param\")\n",
    "    F_Mode = Parameter(\"Define Forecast Mode : week/month\", \"month\")\n",
    "    im = Parameter(\"Outlier Imputation Method : mean/median/mode/quartile value(qv=0.95)\", \"median\")\n",
    "    ot = Parameter(\"Outlier Threshold\", \"1.5\")\n",
    "    ofp = Parameter(\"Outlier Imputation Flag (set yes if Product Level)(if yes then put global flag be false)\", True)\n",
    "    gs = Parameter(\"\"\"Global Imputation Smoothing (set yes if you have atleast one off product with very high values)\n",
    "                    (if yes then put outlier flag be false)\"\"\", False)\n",
    "    \n",
    "    \n",
    "    a = tsk.t1.get_df(q, idp)\n",
    "    get_para = tsk.t1.frcst_preprocessing(a, k, d, v, F_Mode, ofp, im, ot, gs)\n",
    "    fore = tsk.t1.autoArimaForecastParams(get_para, idp, tabel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce219ba9-6333-4da4-9b16-57735db06a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The flow autoArimaForecastParams has been registered successfully. Latest version available now is 0.3 !!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.register(libraryName = \"Forecasting_v1\",flowName =\"autoArimaForecastParams\",description = \"to do autoArima Paramater check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1aea731-881a-4701-88a1-8ebf26ca637a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flows loaded !!\n"
     ]
    }
   ],
   "source": [
    "flw.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb6e1c5b-a204-4dc9-b162-ec07165282c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Version 0.3 for the flow autoArimaForecastParams has been promoted to P successfully !!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flw.Forecasting_v1.autoArimaForecastParams.promote(\"P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01a43327-a0e2-4434-b9a1-2c4b215e9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flw.Forecasting_v1.autoArimaForecastParams.deleteVersion(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351aa74d-9b5b-47cc-a8d9-e74ac8c44487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
